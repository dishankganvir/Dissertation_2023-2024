{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af5f1d3-794d-4dc9-ba22-c55d0bde9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pycocoevalcap in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (1.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from pycocoevalcap) (2.0.7)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\.conda\\envs\\cuda-ev\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c5e924-ce9a-4aa3-a63a-5e0e12d526dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, BertTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.meteor.meteor import Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e4a431-8a74-4c4b-8163-31b583abc8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained model and processor\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"atasoglu/vit-bert-flickr8k\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"atasoglu/vit-bert-flickr8k\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"atasoglu/vit-bert-flickr8k\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062c3a8b-dd9b-42fb-a780-a4536cb25572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load captions\n",
    "def load_captions(captions_file):\n",
    "    image_captions = {}\n",
    "    with open(captions_file, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('#', 2)\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            img_id, _, caption = parts\n",
    "            if img_id not in image_captions:\n",
    "                image_captions[img_id] = []\n",
    "            image_captions[img_id].append(caption)\n",
    "    return image_captions\n",
    "\n",
    "# Function to generate a caption for a single image\n",
    "def generate_caption(image_path, max_new_tokens=50):\n",
    "    img = Image.open(image_path)\n",
    "    pixel_values = feature_extractor(images=[img], return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    output_ids = model.generate(pixel_values, max_new_tokens=max_new_tokens)\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f82bcf-9fe4-427c-b816-c150463262f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_captions(generated_captions, reference_captions):\n",
    "    # Ensure references are structured as a list of lists, where each sublist contains tokenized reference captions\n",
    "    refs = [[ref.split() for ref in refs_for_image] for refs_for_image in reference_captions]\n",
    "    hyps = [gen_caption.split() for gen_caption in generated_captions]\n",
    "\n",
    "    # Ensure the number of hypotheses matches the number of reference sets\n",
    "    assert len(hyps) == len(refs), \"The number of hypotheses and reference sets should be the same.\"\n",
    "\n",
    "    # Apply smoothing function to BLEU score\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(refs, hyps, smoothing_function=smoothing_function)\n",
    "\n",
    "    # METEOR Score\n",
    "    meteor = Meteor()\n",
    "    meteor_score, _ = meteor.compute_score({i: [\" \".join(ref) for ref in refs[i]] for i in range(len(refs))}, {i: [\" \".join(hyps[i])] for i in range(len(hyps))})\n",
    "\n",
    "    # ROUGE Score\n",
    "    rouge = Rouge()\n",
    "    rouge_score, _ = rouge.compute_score({i: [\" \".join(ref) for ref in refs[i]] for i in range(len(refs))}, {i: [\" \".join(hyps[i])] for i in range(len(hyps))})\n",
    "\n",
    "    # CIDEr Score\n",
    "    cider = Cider()\n",
    "    cider_score, _ = cider.compute_score({i: [\" \".join(ref) for ref in refs[i]] for i in range(len(refs))}, {i: [\" \".join(hyps[i])] for i in range(len(hyps))})\n",
    "\n",
    "    return bleu_score, meteor_score, rouge_score, cider_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752e8f37-c9e6-4b95-9b3b-9d4920042ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Path to your dataset\n",
    "image_folder = r'C:\\Users\\User\\Workbooks\\Dissertation\\Flickr8k_Dataset\\Flicker8k_Dataset'\n",
    "captions_file = r'C:\\Users\\User\\Workbooks\\Dissertation\\Flickr8k_text\\Flickr8k.token.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9db5c63-f83d-4f76-8dfc-ac768595fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions\n",
    "image_captions = load_captions(captions_file)\n",
    "\n",
    "# Select 5 random images\n",
    "image_files = list(image_captions.keys())\n",
    "random_image_files = random.sample(image_files, 5)\n",
    "\n",
    "# Generate captions for the selected images\n",
    "image_paths = [os.path.join(image_folder, img_file) for img_file in random_image_files]\n",
    "generated_captions = [generate_caption(image_path) for image_path in image_paths]\n",
    "\n",
    "# Reference captions for evaluation (list of lists, where each list corresponds to an image)\n",
    "reference_captions = [image_captions[img_file] for img_file in random_image_files]\n",
    "\n",
    "# Evaluate captions\n",
    "bleu_score, meteor_score, rouge_score, cider_score = evaluate_captions(generated_captions, reference_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc8dd6-8a28-4173-86cb-5dc3bb6d4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "print(f\"METEOR Score: {meteor_score}\")\n",
    "print(f\"ROUGE Score: {rouge_score}\")\n",
    "print(f\"CIDEr Score: {cider_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7e265-96b6-4207-bd8e-24beb89a39bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
